## About
This repository contains few exercised done using Spark on Python. All the code should be executable as long as the computer meets the requirements mentioned in the dependencies section.

## Dependencies
To execute my projects you will need a system that satisfies below dependencies. These projects were done on Linux machine so you can use Linux Ubuntu, AWS EC2, AWS EMR (Elastic MapReduce) or any cluster distributed computing environment that has spark.
- Python 3.5
- Spark 2.1
- Scala
- Java
- Linux(Ubuntu, AWS EC2, AWS EMR, Notebook)

## Projects
### Spark DataFrame API
- The code for spark DataFrameAPI can be found [here.](https://github.com/santoshblearner/ds-ml-portfolio/blob/master/Spark%20MLlib/Spark_DataFrame_API_Project/Spark_DataFrames_API.ipynb)

### Walmart Data Analysis on Spark
- The code and project for Walmart data analysis can be found [here.](https://github.com/santoshblearner/ds-ml-portfolio/blob/master/Spark%20MLlib/Spark_Walmart_Data_Analysis_Project/Spark%20Walmart%20Data%20Analysis%20Project.ipynb)

## More Information 
- More information about PySpark and programming Spark using Python can be found [here.](https://spark.apache.org/docs/0.9.1/python-programming-guide.html)
- More information about Spark can be found [here.](http://spark.apache.org/)

### Spark Cluster Overview
![Alt](https://spark.apache.org/docs/latest/img/cluster-overview.png)


## References
- [Apache Spark](https://spark.apache.org/)
- [Media Licdn](https://media.licdn.com/)
- [Pieraian Data](http://www.pieriandata.com/)
- [SparkSQL Tutorial](https://www.tutorialspoint.com/spark_sql/spark_sql_quick_guide.htm)
